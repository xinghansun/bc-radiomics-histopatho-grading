
# 基于医学影像的乳腺癌组织学分级分类研究综述

乳腺癌的组织学分级（I级、II级、III级）是评估肿瘤侵袭性和预后的重要病理指标。近年来，研究者尝试利用医学影像（如钼靶X线乳房摄影MG、超声US、磁共振MRI及其动态增强DCE-MRI）进行组织学分级的无创预测。下面将从单模态/多模态方法、放射组学与深度学习、迁移学习、融合策略、模型训练验证、分类任务设定、评估指标及模型复杂度等方面，对当前研究进行综述，并在此基础上提出由易到难的三个潜在研究方向。

## 单模态与多模态乳腺影像分级方法

**钼靶X线（MG）**：乳腺钼靶成像广泛用于筛查和诊断，一些研究利用MG的影像特征预测肿瘤级别。传统方法常基于放射组学，从MG中提取肿块的形状、边缘、密度等特征，结合机器学习分类器预测分级。例如，有研究者构建了基于MG影像的放射组学列线图模型，以预测浸润性导管癌的组织学分级，验证集上AUC约为0.750 ( [Preoperative Prediction of Breast Cancer Histological Grade Using Intratumoral and Peritumoral Radiomics Features from T2WI and DWI MR Sequences - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11668253/#:~:text=,PMC%20free%20article) ) ( [Preoperative Prediction of Breast Cancer Histological Grade Using Intratumoral and Peritumoral Radiomics Features from T2WI and DWI MR Sequences - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11668253/#:~:text=match%20at%20L646%20mammography,However%2C%20the%20performances%20of) )。这表明MG的纹理和形态学特征对分级具有一定区分能力，但单一模态的性能可能有限。

**超声（US）**：超声成像可提供肿瘤形态和内部回声信息。一些多中心研究利用超声放射组学结合临床因素预测乳腺癌级别。例如，Ge等人（2024）从超声图像分割肿瘤ROI，提取788个纹理特征，经过Boruta算法筛选出7个最佳特征，使用逻辑回归模型区分低级别（I+II）和高级别（III）癌症，在内部和外部验证集上AUC分别达到0.731和0.738 ([Noninvasive Assessment of Tumor Histological Grade in Invasive Breast Carcinoma Based on Ultrasound Radiomics and Clinical Characteristics: A Multicenter Study - PubMed](https://pubmed.ncbi.nlm.nih.gov/38780506#:~:text=was%20developed,model%20were%20able%20to%20predict))。当加入肿瘤大小等临床因素构建联合模型时，外部验证AUC约为0.737，与纯影像模型相当 ([Noninvasive Assessment of Tumor Histological Grade in Invasive Breast Carcinoma Based on Ultrasound Radiomics and Clinical Characteristics: A Multicenter Study - PubMed](https://pubmed.ncbi.nlm.nih.gov/38780506#:~:text=from%20788%20candidate%20features,which%20may%20enable%20tailored%20therapeutic))。可见超声放射组学能一定程度上预测组织学级别，但性能中等。深度学习方面，由于超声图像噪声大、伪差多，直接用CNN分类分级的研究相对较少。不过，有研究表明通过序列帧信息提升超声诊断性能：例如将超声视频序列输入ConvNeXt网络提取深度特征，并引入基于图像质量的评分池化，提高了良恶性判别准确率至约85%以上 ( [Predicting Breast Tumor Malignancy Using Deep ConvNeXt Radiomics and Quality-Based Score Pooling in Ultrasound Sequences - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9139635/#:~:text=also%20outperforms%20the%20results%20of,score%20based%20on%20this%20analysis) )。这一策略暗示深度模型有潜力捕捉超声序列中的细微差异，同样可拓展用于预测组织学等级。

**磁共振（MRI）**：MRI尤其是DCE-MRI能提供肿瘤血供、渗透等功能信息，因而与肿瘤分级关联密切。大量研究集中于MRI的放射组学分析和深度学习建模。例如，Wang等人（2022）提取MRI影像放射组学特征，通过逻辑回归构建分级模型，将I级和II级归为低级别，与III级高别进行二分类，其验证集AUC达到0.722 ( [Development and Validation of an MRI Radiomics-Based Signature to Predict Histological Grade in Patients with Invasive Breast Cancer - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9574565/#:~:text=Of%20the%20901%20patients%2C%20618,radiomics%20model%E2%80%99s%20clinical%20application%20value) )。另一项研究比较了不同MRI参数对分级的作用，发现扩散加权成像的ADC值及其纹理特征对区分高级别癌有显著作用：基于ADC的放射组学模型AUC达0.869，明显优于单纯ADC值模型（AUC 0.711）和DCE纹影模型（AUC 0.682） ([Value of magnetic resonance imaging radiomics features in predicting histologic grade of invasive ductal carcinoma of the breast - PubMed](https://pubmed.ncbi.nlm.nih.gov/38393931/#:~:text=with%20Grades%20I%20and%20II,the%20DCE%20radiomics%20signature%20model))。这说明MRI多参数成像提供的定量特征有助于提高分级准确性。与此同时，深度学习也被用于MRI分级：Sun等人（2023）提出了一个针对DCE-MRI的两阶段深度学习框架，第一阶段用改进的卷积神经网络（含空洞空间金字塔池化和Inception模块）提取肿瘤表观特征并初步预测级别，第二阶段融合分子亚型信息进行细调，并对多时相MRI序列采用集成学习获取最终等级 ([Molecular-subtype guided automatic invasive breast cancer grading using dynamic contrast-enhanced MRI - PubMed](https://pubmed.ncbi.nlm.nih.gov/37716219#:~:text=Methods%3A%20%20In%20Stage%20I%2C,Objective%20assessment%20is%20quantitatively%20evaluated)) ([Molecular-subtype guided automatic invasive breast cancer grading using dynamic contrast-enhanced MRI - PubMed](https://pubmed.ncbi.nlm.nih.gov/37716219#:~:text=integrated%20deep%20vectors%20from%20IOS%5E%7B2%7D,0.05))。该模型在区分三类组织学级别上取得了**92.7%**的准确率，AUC高达**0.927**（95%置信区间0.908–0.946） ([Molecular-subtype guided automatic invasive breast cancer grading using dynamic contrast-enhanced MRI - PubMed](https://pubmed.ncbi.nlm.nih.gov/37716219#:~:text=Results%3A%20%20The%20molecular,weighted))。如此高的性能得益于充分利用了MRI时间序列信息和辅助的分子分型信息，展示了深度学习在MRI分级中的巨大潜力。

**多模态融合**：单一成像模态往往只反映肿瘤某一方面特征，为进一步提高预测性能，研究者探索将多种影像信息融合用于分级。常见组合包括MRI+DCE参数、MRI+超声、MG+超声等。放射组学方面，有研究分别从超声和MRI提取特征，然后采用晚期融合策略（特征级拼接或预测概率平均）建立联合模型。结果显示多模态模型优于单模态：如将超声和MRI的最佳纹理特征输入逻辑回归，可将良恶性分类AUC从单一超声的0.82和单一MRI的0.85提高到融合后的0.91 ( [The diagnostic value of multimodal imaging based on MR combined with ultrasound in benign and malignant breast diseases - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11116236/#:~:text=ultrasound%20combined%20with%20MR%20Image,benign%20and%20malignant%20breast%20diseases) )。在组织学分级任务上，多模态放射组学也展现出优势。有学者将MRI的解剖结构信息（T2WI）与功能信息（DWI）相结合，分别提取肿瘤内部和周围3mm环绕区域的特征，建立多种组合模型。结果表明：“T2WI+DWI*肿瘤+周围”综合特征的模型效果最佳，训练集AUC达到0.860，独立测试集AUC提高到0.781，优于任何单一序列或仅肿瘤内部的模型 ( [Preoperative Prediction of Breast Cancer Histological Grade Using Intratumoral and Peritumoral Radiomics Features from T2WI and DWI MR Sequences - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11668253/#:~:text=on%20T2WI%20and%20DWI,860%20for%20the) )。相比之下，以往单纯基于钼靶影像的分级模型AUC约为0.75 ( [Preoperative Prediction of Breast Cancer Histological Grade Using Intratumoral and Peritumoral Radiomics Features from T2WI and DWI MR Sequences - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11668253/#:~:text=match%20at%20L646%20mammography,However%2C%20the%20performances%20of) )；而多参数、多区域特征融合后，性能有明显提升。这些研究表明，多模态晚期融合策略（在特征或决策层融合不同模态的信息）有助于挖掘肿瘤的多重表征，提高组织学等级预测的准确率。

## 放射组学与深度学习方法对比

([Noninvasive Assessment of Tumor Histological Grade in Invasive Breast Carcinoma Based on Ultrasound Radiomics and Clinical Characteristics: A Multicenter Study - PubMed](https://pubmed.ncbi.nlm.nih.gov/38780506)) _放射组学与深度学习分析乳腺影像预测组织学分级的流程示意图。上半部分展示了典型放射组学管线：先在影像上分割出肿瘤ROI（如图中红色区域），提取大量人工设计的特征（一阶统计量、形状特征、纹理矩阵等），再经过特征筛选（如一致性检验、统计检验）挑选出与肿瘤级别相关的特征用于模型训练（常用分类器包括逻辑回归LR、支持向量机SVM、随机森林RF、XGBoost等）。下半部分显示模型构建与性能评估流程，包括训练预测模型并输出组织学等级，以及采用交叉验证、绘制ROC曲线计算AUC、准确率ACC、敏感性SEN、特异性SPE、阳性预测值PPV、阴性预测值NPV等指标评价模型，必要时还可结合决策曲线分析（DCA）和列线图（NOM）等工具。_ ([Noninvasive Assessment of Tumor Histological Grade in Invasive Breast Carcinoma Based on Ultrasound Radiomics and Clinical Characteristics: A Multicenter Study - PubMed](https://pubmed.ncbi.nlm.nih.gov/38780506#:~:text=Figure%202)) ([Noninvasive Assessment of Tumor Histological Grade in Invasive Breast Carcinoma Based on Ultrasound Radiomics and Clinical Characteristics: A Multicenter Study - PubMed](https://pubmed.ncbi.nlm.nih.gov/38780506#:~:text=Figure%203))

传统**放射组学**方法的优点是特征具有可解释性，模型通常是简单可控的机器学习算法，在小样本数据集上也能训练稳定 ([Noninvasive Assessment of Tumor Histological Grade in Invasive Breast Carcinoma Based on Ultrasound Radiomics and Clinical Characteristics: A Multicenter Study - PubMed](https://pubmed.ncbi.nlm.nih.gov/38780506#:~:text=%28AUC%29%20values%20of%200,BC%20in%20routine%20clinical%20use))。多个研究已证明放射组学特征（如纹理、形状）与乳腺癌的组织学分级显著相关 ([Value of magnetic resonance imaging radiomics features in predicting histologic grade of invasive ductal carcinoma of the breast - PubMed](https://pubmed.ncbi.nlm.nih.gov/38393931/#:~:text=differences%20in%20radiomics%20features%20between,the%20diagnostic%20efficacy%20was%20evaluated)) ([Value of magnetic resonance imaging radiomics features in predicting histologic grade of invasive ductal carcinoma of the breast - PubMed](https://pubmed.ncbi.nlm.nih.gov/38393931/#:~:text=with%20Grades%20I%20and%20II,the%20DCE%20radiomics%20signature%20model))。例如超声放射组学模型能够区分高级别和低级别癌症（AUC约0.73） ([Noninvasive Assessment of Tumor Histological Grade in Invasive Breast Carcinoma Based on Ultrasound Radiomics and Clinical Characteristics: A Multicenter Study - PubMed](https://pubmed.ncbi.nlm.nih.gov/38780506#:~:text=was%20developed,model%20were%20able%20to%20predict))；MRI放射组学模型也能在一定程度上预测浸润性癌的等级（AUC ~0.72） ( [Development and Validation of an MRI Radiomics-Based Signature to Predict Histological Grade in Patients with Invasive Breast Cancer - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9574565/#:~:text=Of%20the%20901%20patients%2C%20618,radiomics%20model%E2%80%99s%20clinical%20application%20value) )。放射组学还便于结合临床变量构建列线图辅助决策。然而，其缺点在于对ROI分割和特征工程依赖强，需要人工参与且特征覆盖面有限，可能遗漏深层次的影像模式。此外，不同设备与成像参数会影响特征稳定性，模型的泛化能力受到挑战。

**深度学习**方法通过卷积神经网络（CNN）等直接从影像中学习特征表示，避免了手工特征设计。ResNet、DenseNet等经典CNN架构已被广泛应用于乳腺影像分析任务中，实现了从肿块检出、良恶性诊断到分子分型等多种功能 ([Deep learning applications to breast cancer detection by magnetic ...](https://breast-cancer-research.biomedcentral.com/articles/10.1186/s13058-023-01687-4#:~:text=Deep%20learning%20applications%20to%20breast,MRI)) ([Frontiers | Cross-attention multi-branch CNN using DCE-MRI to classify breast cancer molecular subtypes](https://www.frontiersin.org/journals/oncology/articles/10.3389/fonc.2023.1107850/full#:~:text=Results%3A%20Several%20classical%20deep%20learning,age%20at%20menarche%20years%2C%20our))。ResNet系列（如ResNet-50）由于经过大型数据集预训练，能够提取丰富的通用图像特征，常作为乳腺医学影像分类的骨干网络。一些研究将预训练的ResNet迁移用于乳腺癌分级，通过微调最后的全连接层来输出等级预测，提高了小数据集上的学习效率。此外，新近出现的ConvNeXt架构对ResNet进行了现代化改进（如引入Transformers的训练策略等），在乳腺超声肿瘤分类等任务中性能优于EfficientNet、Vision Transformer等模型 ( [Predicting Breast Tumor Malignancy Using Deep ConvNeXt Radiomics and Quality-Based Score Pooling in Ultrasound Sequences - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9139635/#:~:text=vision%20transformer%20%28ViT%29%20,ConvNeXt%20outperforms%20all%20other) )。例如，使用ConvNeXt提取超声图像深度特征相比传统CNN提高了2-3%的准确率 ( [Predicting Breast Tumor Malignancy Using Deep ConvNeXt Radiomics and Quality-Based Score Pooling in Ultrasound Sequences - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9139635/#:~:text=match%20at%20L711%20EfficientNet,higher%20than) ) ( [Predicting Breast Tumor Malignancy Using Deep ConvNeXt Radiomics and Quality-Based Score Pooling in Ultrasound Sequences - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9139635/#:~:text=vision%20transformer%20%28ViT%29%20,ConvNeXt%20outperforms%20all%20other) )。深度学习模型能够自动挖掘复杂非线性的影像模式，结合多模态输入时也更为方便（可设计多分支网络分别处理不同模态）。在乳腺癌组织学分级任务中，深度学习模型已经展现出优于放射组学的性能上限 ([Molecular-subtype guided automatic invasive breast cancer grading using dynamic contrast-enhanced MRI - PubMed](https://pubmed.ncbi.nlm.nih.gov/37716219#:~:text=Results%3A%20%20The%20molecular,weighted))。不过其劣势在于对数据量要求高，训练过程需要大量计算资源（GPU加速），模型调试难度大且可解释性相对较弱。

总体而言，放射组学方法适用于数据较少、需要可解释性的场景，而深度学习在大样本和多模态融合方面更具优势。许多最新研究也尝试将两者结合，如利用深度学习提取的特征作为“深度放射组学”输入传统模型，或者用放射组学特征与CNN提取特征一起融合，提高模型鲁棒性 ( [Predicting Breast Tumor Malignancy Using Deep ConvNeXt Radiomics and Quality-Based Score Pooling in Ultrasound Sequences - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9139635/#:~:text=a%20robust%20BUS%20image%20classification,score%20of%20the%20whole%20sequence) ) ( [Predicting Breast Tumor Malignancy Using Deep ConvNeXt Radiomics and Quality-Based Score Pooling in Ultrasound Sequences - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9139635/#:~:text=3) )。

## 迁移学习与预训练应用

由于医学影像数据有限且获取代价高，**迁移学习**在乳腺癌影像分级模型中非常普遍。典型做法是采用在大型通用图像数据集（如ImageNet）上预训练的CNN模型，然后迁移到乳腺影像任务中。预训练模型已学习到低层次的边缘、纹理等特征，可以帮助提升小样本下的收敛速度和精度。例如，文献中常用的ResNet-50预训练权重可视作强大的特征提取器 ( [Predicting Breast Tumor Malignancy Using Deep ConvNeXt Radiomics and Quality-Based Score Pooling in Ultrasound Sequences - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9139635/#:~:text=We%20employed%20ConvNeXt%20,%E2%80%94and%20stem%20cell) )。研究者只需针对乳腺影像细节进行微调，就能取得比随机初始化训练更好的效果。此外，一些研究在公开医学影像数据集上进行**预训练**或**自监督学习**，如在大规模乳腺X线数据集上预训练ConvNeXt，再迁移用于分级任务，以期提取更具针对性的特征。同样，预训练的好处在于减少对训练样本数量的需求，缓解过拟合。同时也有学者探索多任务学习，将组织学分级与相关任务（如良恶性分类、分子分型预测等）一起训练共享网络，提高特征的通用性。在当前文献中，大部分深度学习模型均采用迁移学习方案，使用公开数据或先验任务进行预训练，然后在乳腺癌分级数据上精调，这已成为提高模型性能和稳健性的关键步骤。

## 多模态融合策略及晚期融合

多模态数据融合是乳腺癌影像研究的热点之一。在组织学分级任务中，不同模态提供互补的信息：例如MG反映钙化和密度，US提供形态和边界，MRI给出内部增强动力学。融合策略大致分为早期融合、中期融合和晚期融合三类。其中**晚期融合**应用较为广泛，尤其在现有文献中多见，其特点是各模态先独立提取特征或得到预测结果，然后在决策层面融合。晚期融合的优点是对各模态的预处理和特征提取可分别优化，而且不要求原始图像配准或尺寸一致。很多放射组学研究采用简单的晚期融合，例如将不同模态模型输出的概率取加权平均，或将各模态选出的特征向量直接拼接输入分类器 ( [The diagnostic value of multimodal imaging based on MR combined with ultrasound in benign and malignant breast diseases - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11116236/#:~:text=best%20features%2C%20including%20ultrasound%20model%2C,67) ) ( [The diagnostic value of multimodal imaging based on MR combined with ultrasound in benign and malignant breast diseases - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11116236/#:~:text=ultrasound%20combined%20with%20MR%20Image,benign%20and%20malignant%20breast%20diseases) )。深度学习中也可以设计多分支网络，各分支对应一种模态，在接近输出的全连接层将分支特征级联，再通过softmax给出最终分级结果。这种架构被称为“晚期特征融合”，已经在乳腺多模态诊断中证明有效（如MRI+超声融合提高诊断AUC约5-9个百分点） ( [The diagnostic value of multimodal imaging based on MR combined with ultrasound in benign and malignant breast diseases - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11116236/#:~:text=ultrasound%20combined%20with%20MR%20Image,benign%20and%20malignant%20breast%20diseases) )。相比之下，早期融合（如将多模态影像在输入层直接堆叠当作多通道）由于图像物理性质差异大、空间对齐困难，较少用于乳腺癌分级。另外还有**中期融合**（如在网络中间层通过注意力机制融合），部分新研究尝试在CNN中引入交叉注意力模块，使不同模态特征在更高层语义上交互，从而提升判别力。然而考虑到实现复杂度和训练难度，目前多数乳腺癌分级研究还是采用策略简单且效果可靠的晚期融合手段。

值得一提的是，多模态不局限于多种影像类型的融合，还包括**多参数多序列融合**（如DCE-MRI的不同时相序列、MRI的T1加权与T2加权融合等）以及**影像与非影像数据融合**（如结合分子亚型、基因信息等）。Sun等人的研究就是将MRI影像特征与分子亚型信息在决策层加权融合，证明可明显提升三分类准确率 ([Molecular-subtype guided automatic invasive breast cancer grading using dynamic contrast-enhanced MRI - PubMed](https://pubmed.ncbi.nlm.nih.gov/37716219#:~:text=IBC%20grades%20and%20performs%20preliminary,0.05)) ([Molecular-subtype guided automatic invasive breast cancer grading using dynamic contrast-enhanced MRI - PubMed](https://pubmed.ncbi.nlm.nih.gov/37716219#:~:text=Results%3A%20%20The%20molecular,weighted))。今后的方向可能是探索更智能的融合策略（如学习自适应的模态权重），以充分利用多源信息提高分级预测性能。

## 模型训练与验证策略

乳腺癌组织学分级任务中常面临**小样本、类别不平衡**等挑战，因此研究中采用了多种训练和验证策略来提高模型泛化能力和可靠性：

- **小样本学习与数据增强**：由于单中心的数据量往往有限，研究普遍使用数据增强技术扩充训练集。例如，对乳腺影像进行随机翻转、旋转、缩放和平移等变换，以模拟不同成像条件并减少模型对特定取向的依赖。一些MRI研究还对图像进行随机裁剪、添加高斯噪声等来增加鲁棒性。对于深度学习模型，小样本情况下通常依赖迁移学习初始化权重，并冻结部分卷积层仅训练高层参数，从而降低过拟合风险。
    
- **交叉验证和独立验证**：为了充分利用数据又保证结果可靠，不少工作采用K折交叉验证训练和测试模型。例如将数据分成5折或10折，轮流训练模型并综合评估平均性能 ([Noninvasive Assessment of Tumor Histological Grade in Invasive Breast Carcinoma Based on Ultrasound Radiomics and Clinical Characteristics: A Multicenter Study - PubMed](https://pubmed.ncbi.nlm.nih.gov/38780506#:~:text=%28AUC%29%20values%20of%200,BC%20in%20routine%20clinical%20use))。交叉验证有助于减小因训练集划分不同带来的结果波动。除了交叉验证，也有研究引入**外部独立验证集**评估模型泛化，如Ge等利用两个中心的数据，其中一家医院数据用于独立测试模型，得到更客观的性能指标 ([Noninvasive Assessment of Tumor Histological Grade in Invasive Breast Carcinoma Based on Ultrasound Radiomics and Clinical Characteristics: A Multicenter Study - PubMed](https://pubmed.ncbi.nlm.nih.gov/38780506#:~:text=histological%20grade%20of%20invasive%20breast,score)) ([Noninvasive Assessment of Tumor Histological Grade in Invasive Breast Carcinoma Based on Ultrasound Radiomics and Clinical Characteristics: A Multicenter Study - PubMed](https://pubmed.ncbi.nlm.nih.gov/38780506#:~:text=from%20788%20candidate%20features,which%20may%20enable%20tailored%20therapeutic))。
    
- **类别不平衡处理**：乳腺癌分级数据中低级（尤其I级）病例往往明显少于II级和III级，这会导致模型训练偏向多数类。为此，常用的策略包括上采样少数类（过采样I级病例或生成合成样本）、下采样多数类，或在损失函数中引入**类别权重**。一些深度学习研究采取改进的损失函数，例如Sun等人设计了针对类别不均衡的F1-score损失，直接优化模型的F1指标，从而提升对少数类别的敏感度 ([Molecular-subtype guided automatic invasive breast cancer grading using dynamic contrast-enhanced MRI - PubMed](https://pubmed.ncbi.nlm.nih.gov/37716219#:~:text=pooling%20layer%20%28DA%29%20and%20inception,DeLong%20test%20is%20applied%20to))。此外，也有工作使用**集成学习**平衡性能，通过训练多个子模型侧重不同类别，再融合它们的结果减少偏差 ([Molecular-subtype guided automatic invasive breast cancer grading using dynamic contrast-enhanced MRI - PubMed](https://pubmed.ncbi.nlm.nih.gov/37716219#:~:text=IBC%20grades%20and%20performs%20preliminary,0.05))。
    
- **训练超参数和早停**：深度学习模型的训练通常设置较小的batch size（例如8或16）以适应GPU显存限制 ([Frontiers | Cross-attention multi-branch CNN using DCE-MRI to classify breast cancer molecular subtypes](https://www.frontiersin.org/journals/oncology/articles/10.3389/fonc.2023.1107850/full#:~:text=processing%20unit%20,we%20chose%20RMSprop%20as))。初始学习率设定后根据验证集性能调整（如每隔若干epoch降低学习率），使用优化算法如RMSprop或Adam ([Frontiers | Cross-attention multi-branch CNN using DCE-MRI to classify breast cancer molecular subtypes](https://www.frontiersin.org/journals/oncology/articles/10.3389/fonc.2023.1107850/full#:~:text=processing%20unit%20,we%20chose%20RMSprop%20as))。同时监测验证集损失，应用**早停机制**防止过拟合。当验证性能不再提升时提前终止训练并保存最佳模型参数。
    

通过上述策略，研究者尽可能确保模型训练充分且不过拟合，小样本下仍具备稳健的泛化性能。例如，有研究在5折交叉验证下将模型测试AUC提升至约0.96，同时不同折性能差异很小 ([Frontiers | Cross-attention multi-branch CNN using DCE-MRI to classify breast cancer molecular subtypes](https://www.frontiersin.org/journals/oncology/articles/10.3389/fonc.2023.1107850/full#:~:text=Results%3A%20Several%20classical%20deep%20learning,recall%20of%2093.33))；另有研究的10折验证证明其放射组学模型结果稳定可重复 ([Noninvasive Assessment of Tumor Histological Grade in Invasive Breast Carcinoma Based on Ultrasound Radiomics and Clinical Characteristics: A Multicenter Study - PubMed](https://pubmed.ncbi.nlm.nih.gov/38780506#:~:text=%28AUC%29%20values%20of%200,BC%20in%20routine%20clinical%20use))。这些实践为我们设计新模型提供了经验借鉴。

## 分类任务设置与评估指标

在组织学分级建模时，**分类任务的定义**可能有所不同。大多数研究将乳腺癌分级视为**二分类问题**，即将I级和II级归为低～中等级，III级作为高级别进行区分 ( [Development and Validation of an MRI Radiomics-Based Signature to Predict Histological Grade in Patients with Invasive Breast Cancer - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9574565/#:~:text=Of%20the%20901%20patients%2C%20618,radiomics%20model%E2%80%99s%20clinical%20application%20value) ) ( [Preoperative Prediction of Breast Cancer Histological Grade Using Intratumoral and Peritumoral Radiomics Features from T2WI and DWI MR Sequences - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11668253/#:~:text=results%20were%20included,because%20there%20were%20relatively%20few) )。这样做的原因是I级病例通常占比极少，合并I、II级可以缓解数据不均衡，同时临床上III级肿瘤生物学行为与I/II级差异较大，二分有明确意义。二分类模型往往着重于检测高度恶性（III级）肿瘤，取得的性能也相对较高，例如MRI放射组学模型能达到约0.8的AUC来识别III级 ( [Preoperative Prediction of Breast Cancer Histological Grade Using Intratumoral and Peritumoral Radiomics Features from T2WI and DWI MR Sequences - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11668253/#:~:text=3mmPTR%2C%20%28T2WI%20%2B%20DWI%29,860%20for%20the) )。也有部分研究尝试**三分类**任务，直接将I、II、III级同时分类。但由于I级样本过少且II级与I或III级的界限不如良恶性那样鲜明，三分类比二分类更具挑战。Sun等人利用深度学习结合亚型信息，实现了I vs II vs III的三分类，其高精度（>92%）证明了多信息融合可以显著改善三分类的可行性 ([Molecular-subtype guided automatic invasive breast cancer grading using dynamic contrast-enhanced MRI - PubMed](https://pubmed.ncbi.nlm.nih.gov/37716219#:~:text=Results%3A%20%20The%20molecular,weighted))。在规划研究时，需要根据数据分布和临床需求决定二分类还是三分类：若I级样本极少或关注临床决策（如是否为高级别需要强化治疗），可优先考虑二分类方案；若数据相对充足且目标在于全面自动分级，可探索三分类网络，但需付出更多模型复杂度和策略来应对类别不均衡问题。

**评估指标**方面，几乎所有研究都会采用**准确率（Accuracy）**和**受试者工作特征曲线下面积（AUC）**来衡量模型总体性能。AUC能够综合反映模型区分阳性（高等级）和阴性（低等级）的能力，尤其在类别不平衡时是一项稳健指标 ([Noninvasive Assessment of Tumor Histological Grade in Invasive Breast Carcinoma Based on Ultrasound Radiomics and Clinical Characteristics: A Multicenter Study - PubMed](https://pubmed.ncbi.nlm.nih.gov/38780506#:~:text=%28AUC%29%20values%20of%200,BC%20in%20routine%20clinical%20use))。同时，**敏感度（召回率, Recall或Sensitivity）**和**特异度（Specificity）**用于评估模型对正负类的识别率；**精确度（Precision）**和**F1-score**考察模型预测的准确性和稳定性。在医学背景下，高级别肿瘤往往是关注的“阳性”类，因此研究常报告针对III级的敏感度（召回）和针对低级别的特异度等。有些文献还提供**混淆矩阵**以展示各等级分类正确与错误的数量分布，从而发现模型易混淆的等级对。例如，可能模型容易将II级错分为I或III级，这是常见难点。若进行了二分类，则计算阳性类（III级）的**阳性预测值PPV**和**阴性预测值NPV**来体现模型在不同结果下的可信度 ([Noninvasive Assessment of Tumor Histological Grade in Invasive Breast Carcinoma Based on Ultrasound Radiomics and Clinical Characteristics: A Multicenter Study - PubMed](https://pubmed.ncbi.nlm.nih.gov/38780506#:~:text=as%20Rad,BC%20in%20routine%20clinical%20use))。此外，**Kappa系数**或**McNemar检验**用于统计评价模型的一致性，**决策曲线分析（DCA）**用于评估模型在不同阈值下的临床净获益 ( [Preoperative Prediction of Breast Cancer Histological Grade Using Intratumoral and Peritumoral Radiomics Features from T2WI and DWI MR Sequences - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC11668253/#:~:text=accuracy%2C%20and%20AUC%20of%2080.4,compared%20to%20the%20other%20models) )。总的来说，目前最常引用的还是AUC和准确率：多数MRI/超声放射组学模型的AUC在0.7~0.85之间 ([Value of magnetic resonance imaging radiomics features in predicting histologic grade of invasive ductal carcinoma of the breast - PubMed](https://pubmed.ncbi.nlm.nih.gov/38393931/#:~:text=with%20Grades%20I%20and%20II,the%20DCE%20radiomics%20signature%20model)) ([Noninvasive Assessment of Tumor Histological Grade in Invasive Breast Carcinoma Based on Ultrasound Radiomics and Clinical Characteristics: A Multicenter Study - PubMed](https://pubmed.ncbi.nlm.nih.gov/38780506#:~:text=from%20788%20candidate%20features,which%20may%20enable%20tailored%20therapeutic))；而先进的多模态深度学习模型报告的AUC可超过0.90 ([Molecular-subtype guided automatic invasive breast cancer grading using dynamic contrast-enhanced MRI - PubMed](https://pubmed.ncbi.nlm.nih.gov/37716219#:~:text=Results%3A%20%20The%20molecular,weighted))。需要注意直接不同研究之间结果对比的意义有限，因为数据集和任务定义不尽相同，但总体趋势表明融合更多信息和更复杂模型能够逐步提高分级预测的性能。

## 模型复杂度、复现难度与计算资源需求

随着方法从传统机器学习发展到深度学习，模型的复杂度和对计算资源的需求显著增加。放射组学模型通常使用数十到数百维特征、简单分类器，训练在普通电脑上即可完成，计算开销主要在于特征提取。但深度学习模型往往包含数百万甚至上亿参数，需要利用GPU进行加速训练。如前所述，有研究使用了NVIDIA GTX 2060级别的GPU进行模型训练，将batch size设为8、训练200个epoch，每10个epoch降低学习率，整个过程才能收敛得到稳定结果 ([Frontiers | Cross-attention multi-branch CNN using DCE-MRI to classify breast cancer molecular subtypes](https://www.frontiersin.org/journals/oncology/articles/10.3389/fonc.2023.1107850/full#:~:text=processing%20unit%20,we%20chose%20RMSprop%20as))。更复杂的网络（如3D CNN、Transformer架构）可能需要更高端的GPU或多卡并行才能在合理时间内完成训练。另外，训练一个多模态深度模型也需调试各分支网络结构、融合层次和超参数，这对研究团队的编程实现和调参能力提出了较高要求。模型**复现难度**也因此上升：许多深度学习研究缺乏公开代码，加之需要特定的硬件环境和大量训练时间，令他人很难完全重复其结果。这也凸显了详细报告模型参数和提供代码的重要性。在投稿高水平SCI论文时，审稿人可能关注模型的可解释性和复现性，因此如何在保证模型性能的同时控制复杂度、提供合理的可解释模块（如可视化热力图、特征重要性分析）也是需要权衡的。总的来说，目前简单模型易于复现但性能有限，复杂模型效果更好但实现门槛高、资源耗费大。因此研究者在创新方法时应尽量提供详尽的方法描述和开源支持，并评估所需的计算资源（如GPU类型、内存占用、训练时长）确保方案具有实践可行性。

# 基于我们私人数据的潜在研究方向（由易到难）

结合以上综述和本地未标注原始影像数据情况（包含MG、US和MRI多模态影像以及对应病理分级标签），我这里提出三个递进的研究方案方向，每个方向的思路由易到难，并针对研究目标、数据处理、模型设计、实现资源、训练策略、评估指标及创新难点等进行说明。

## 研究方向一：**单模态深度学习模型用于乳腺癌分级（二分类，高级别 vs. 低级别）**

- **研究思路与目标**：从最简单入手，以单一影像模态预测乳腺癌组织学分级。选择一种最能表征肿瘤异质性的成像（例如DCE-MRI的峰值强化相），将分级任务简化为**二分类**（III级 vs. I+II级），重点识别出高级别肿瘤。目标是在小数据条件下验证深度学习模型对肿瘤分级的可行性，期望性能优于传统放射组学基线。
    
- **数据预处理与样本组合策略**：利用原始未标注影像，先由放射科医师或算法检测获得肿瘤的ROI区域。在MRI上可借助分割模型或阈值法圈出肿块轮廓；MG和US上亦可通过传统图像处理或预训练检测模型定位病灶。基于ROI进行剪裁，得到**肿瘤局部图像**，减少背景噪声干扰 ([Frontiers | Cross-attention multi-branch CNN using DCE-MRI to classify breast cancer molecular subtypes](https://www.frontiersin.org/journals/oncology/articles/10.3389/fonc.2023.1107850/full#:~:text=Figure%C2%A01%20Data%20set%20processing%20process,the%20four%20points%20to%20crop))。考虑到本方向聚焦单模态，可选择病例数较多且图像质量高的模态作为训练集。如果多模态数据都有，优先MRI（如DCE早期增强相）或超声（恶性肿瘤在超声上征象差异明显）。在样本组合上，将I级和II级病例归为一类，III级为一类，统计两类样本数。如类别失衡（例如III级远少于I+II级），可通过**过采样**少数类（如旋转/平移产生变体）来平衡训练集。划分训练集和测试集时采用分层抽样，确保两类比例一致。此外，可留出一小部分数据作为验证集调整模型超参数。
    
- **模型结构与融合策略**：采用**单模态CNN分类模型**。基础架构选用经典的**ResNet-50**或更轻量的**ResNet-18**，利用其在ImageNet上预训练的权重进行初始化 ( [Predicting Breast Tumor Malignancy Using Deep ConvNeXt Radiomics and Quality-Based Score Pooling in Ultrasound Sequences - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9139635/#:~:text=We%20employed%20ConvNeXt%20,%E2%80%94and%20stem%20cell) )。将预处理得到的ROI影像缩放到统一尺寸（例如224×224像素），作为模型输入的三通道图像（灰度图像可复制到3通道）。模型的最后输出层修改为二分类（带有softmax的2单元全连接层）。由于仅单一模态，无需复杂融合机制。可以在ResNet主干后接1-2层全连接层以增加模型非线性表达能力。若采用MRI影像，可考虑将肿瘤的T1增强相和T2WI作为两个并行输入分支，在倒数第二层将ResNet提取的特征向量拼接（这属于**中晚期融合**的简单形式），以微融合多序列信息，但本方向的主要融合还是集中在单模态内部。例如对DCE-MRI的不同时间点做简单平均或选取关键帧，不引入多模态融合。
    
- **PyTorch实现路径与所需计算资源评估**：在实现上，利用PyTorch框架的预训练模型库直接加载ResNet-50结构和参数。编写数据加载代码，将ROI图像及标签读入并实时执行数据增强（如随机翻转、裁剪）。模型定义采用torchvision提供的ResNet结构，修改最后的fc层输出2类。训练时使用单GPU即可（如NVIDIA RTX 2080级别显卡），预计占用约几GB显存（ResNet-50对224图像batch size 16占用<8GB）。训练时间根据样本量而定，例如500张训练图像跑100个epoch约需1小时。使用**Adam优化器**，初始学习率设为1e-4，训练过程中监控验证集Loss动态调整。资源方面，本方案计算需求相对低，可在单机GPU上完成，适合作为探索性的基础实验。
    
- **模型训练策略**：采用**迁移学习+微调**策略。首先冻结ResNet的大部分卷积层，仅训练最后的全连接层和顶层卷积层，让模型快速适应新任务特征。观察训练曲线，如验证集精度在数epoch内停滞，则逐步解冻更多层并降低学习率微调。在数据增强方面，对ROI图像进行水平垂直翻转、轻微旋转（±10度）、亮度和对比度扰动，以增广有限的数据。引入**早停机制**，如果验证集AUC在连续10个epoch不提升，则提前停止训练。针对类别不平衡问题，在损失函数（交叉熵）中为高级别类设置更高权重（如根据1:3的样本比给予相反比例权重），或采用**Focal Loss**以降低易分类样本的损失权重，从而促使模型关注难分的高级别病例。整个训练过程中，通过K折交叉验证获取模型稳健的性能估计，并保存每折的最佳模型。
    
- **模型评估方法与预期结果**：在测试集中评估模型分类性能。由于是二分类，可生成ROC曲线并计算AUC，预期AUC有望达到0.80左右，优于放射组学的0.7水平 ([Noninvasive Assessment of Tumor Histological Grade in Invasive Breast Carcinoma Based on Ultrasound Radiomics and Clinical Characteristics: A Multicenter Study - PubMed](https://pubmed.ncbi.nlm.nih.gov/38780506#:~:text=from%20788%20candidate%20features,which%20may%20enable%20tailored%20therapeutic))。同时给出模型的准确率、召回率、特异度和F1-score等指标。希望在**III级**肿瘤的召回率达到较高水平（>85%），确保大部分高级别能被识别；而对于低级别（I+II）的特异度也应在80%以上，使误判率可控。输出混淆矩阵分析错误模式，例如可能部分II级被误判为III级。还可以对训练完的模型采用Grad-CAM等可视化技术生成**热力图**，以了解模型关注的区域是否集中在肿瘤内部。这将提升结果的可解释性。若模型性能接近参考文献中的MRI放射组学模型（AUC约0.72 ( [Development and Validation of an MRI Radiomics-Based Signature to Predict Histological Grade in Patients with Invasive Breast Cancer - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9574565/#:~:text=Of%20the%20901%20patients%2C%20618,radiomics%20model%E2%80%99s%20clinical%20application%20value) )），就证明深度学习方法的可行；若明显更高（逼近0.85），则表明单模态CNN已能较好捕捉影像与分级的关联。
    
- **当前方法的难点与创新点**：本方向难点在于**小数据下深度模型的有效训练**。需解决样本少、类别不均衡导致的过拟合和偏差问题，这也是创新点之一——通过精心的迁移学习和数据增广策略来克服数据限制。另一个难点是ROI获取的准确性：如果肿瘤定位不准将影响模型判别，这方面可以创新地引入现成检测模型或半自动分割来提高ROI质量。创新点还包括将传统放射组学知识融入CNN，如在模型输入前计算一些简单纹理参数作为附加通道信息供CNN参考。总体而言，方向一作为基线，将为更复杂方案打下基础，其创新意义在于验证**深度学习二分类模型在乳腺癌组织学分级上的有效性**，并总结小样本条件下的最佳实践。
    

## 研究方向二：**多模态晚期融合模型用于乳腺癌组织学分级（三分类）**

- **研究思路与目标**：在方向一取得初步成果后，进一步提升模型性能与适用性，引入**多模态影像融合**并尝试直接进行三分类预测I、II、III级别。研究思路是利用不同成像模态的互补信息，通过晚期融合策略提高对中间级别（特别是II级）的判别力，从二分类扩展到更具挑战的三分类任务。目标是在多模态数据集上建立一个融合模型，相较单模态模型明显提高准确率和稳健性，力争在独立验证中实现接近临床可用的分级准确度。
    
- **数据预处理与样本组合策略**：整合MG、US、MRI三种模态的数据。对于每个患者，确保获取到对应病灶的三种影像（需要配准不同模态的病灶位置，或者由放射科专家提供病灶位置标记）。数据预处理包括：分别在每种模态上**剪裁ROI或获取全图病灶patch**。例如，MG上截取包含肿块的子图，US上用分割得到肿瘤区域，MRI上选取最能体现肿瘤的切片或将3D肿瘤进行多角度投影。由于三模态图像分辨率和尺寸不同，可分别处理后缩放至统一输入大小。考虑到三分类需要较多数据支撑，将I、II、III级病例全部用于模型训练。但由于I级样本往往很少，可采取**数据重采样**策略：对I级样本进行增强（例如不同角度的翻转、对比度调节等）以增加其出现频次；或者引入**合成少数类样本**（如利用GAN根据已知I级特征生成虚拟影像）。同时，为每个病例建立多模态配对——即确保MG、US、MRI图像按照病理ID匹配，这样划分数据集时以病例为单位进行K折交叉验证，避免信息泄漏。样本分割上，考虑分层抽样保持各等级比例，也确保各折中三种模态的影像数目一致。
    
- **模型结构与融合策略**：设计**多分支卷积神经网络**实现多模态的特征提取和融合。每种模态对应一个并行的子网络，例如：MG子网使用ResNet-18 backbone（预训练权重微调），US子网使用轻量级的ConvNeXt-Tiny或EfficientNet（考虑到超声纹理特征不同，也可尝试更浅层网络避免过拟合），MRI子网使用ResNet-50或更深的DenseNet（充分提取MRI丰富信息）。各子网的输入为对应模态的ROI图像，提取到的特征在网络的**倒数第二层**输出一个特征向量（长度比如256维）。然后采用**晚期特征融合**：将MG、US、MRI的特征向量**拼接**成一个总的特征向量（长度约768维）。接着通过一个全连接层或小型多层感知机（MLP）对融合特征进行处理，输出3个神经元对应3个等级的预测，并用softmax得到三分类概率。这种融合属于典型的后期融合，在决策之前简单地整合各模态信息。考虑到不同模态对分级的贡献可能不同，可在拼接前对每个模态特征施加一个可学习的**权重系数**（例如引入一个参数向量对每支路特征逐元素缩放），以赋予模型自适应调整模态重要性的能力。如果实现上方便，也可以融入**注意力机制**：例如在拼接后的特征经过自注意力模块，让模型自行关注不同模态特征中与目标类别最相关的部分，从而提升判别效果。但初步阶段可先尝试简单拼接融合以降低复杂度。
    
- **PyTorch实现路径与计算资源评估**：在PyTorch中实现多分支网络可以有多种方式：一种是自定义一个模型类，在`forward`中分别通过三个子网络得到特征，再concat输出。也可以利用`torch.nn.ModuleList`将预定义的单模态模型实例化三个。具体实现时，可加载预训练的ResNet和EfficientNet模型，通过修改其最后几层使之输出指定长度的特征向量。例如截断ResNet在avgpool层后，加上一层Linear将2048维特征降到256维作为子网输出。然后将三个输出在dim=1拼接，通过一个Linear(768→3)实现分类。训练时需要注意协调三个子网的学习率，可给参数组设定不同学习率（例如预训练部分略低，新加的融合层略高）。资源方面，多模态模型参数量是单模态之和，显存占用增加。如果单模态ResNet-50需显存约4GB三者则约12GB，计算量增加会延长训练时间。建议使用**性能较强的GPU**（如NVIDIA RTX 3090 24GB）以同时容纳三分支的计算。若资源有限，也可分阶段训练：先分别训练/微调每个子网（单模态微调），再冻结部分低层参数仅训练融合层，以减少显存占用和加速收敛。此外，可利用**混合精度训练**(FP16)节省显存。估计在1000例数据、batch size 8情况下，训练100个epoch可能需要数小时到十余小时，具体视GPU性能而定。由于引入新模块，代码实现和调试复杂度增大，需要逐步验证每个子网络输出正确再进行融合部分训练。
    
- **模型训练策略**：首先对每个模态的子网分别进行**预训练微调**：可以使用方向一类似的方法，在各自模态的数据上训练二分类或三分类（如果数据够多）模型，使其学到初步分级特征。这相当于给多模态模型一个较好的初始化。【可选】在微调子网时，也可以利用其它公开数据集（如DDSM乳腺X线数据、BUSI超声数据）预训练，但需注意任务差异。随后，将微调后的子网组合成多模态模型进行联合训练。在联合训练时，可采用**分阶段训练**：开始时冻结各子网的大部分层，仅训练融合层和少数顶层参数，以稳定融合过程；然后逐步解冻子网，让模态特征在融合环境下进一步调整。训练过程中继续使用丰富的数据增强策略，但要确保对应病例的不同模态图像增强后的标签一致（如不会因增强导致配准错误）。采用**学习率调度**策略，例如余弦退火或ReduceLROnPlateau，根据验证集三分类AUC或F1表现自动调整学习率。为应对三分类的不均衡，可使用**加权交叉熵损失**（为每级类别设置权重，I级最高、II级次之、III级最低，权重反比于频数）或采用“一级 vs 其余”多个二分类子损失的方式。同时利用**混淆矩阵分析**发现模型对于II级的混淆情况，必要时在损失中加入针对性项（例如提高将II级判对的奖励）。训练评估可使用**多类别F1-score**或**Kappa系数**作为监控指标，确保模型在各类别都有较好表现，而非只偏向多数类。最后，通过K折交叉验证获取模型平均性能，并保存表现最优的一折模型用于最终评估或应用。
    
- **模型评估方法与预期结果**：模型评估涵盖**三分类**的各项指标。首先计算总体准确率，希望能较单模态模型有明显提高（例如由70%提升到80%以上）。绘制每一级别的一对多ROC曲线（如III级 vs 非III级），计算每一类的AUC值，以及宏平均的AUC。预期III级 vs 其他的AUC可达0.90左右，I级 vs 其他由于样本少或表现稍低但争取>0.8，宏平均AUC力争在0.85以上。如果采用One-vs-Rest方法，可得到每类的Precision、Recall和F1，其中重点关注II级的F1值，应比单模态有所提升（例如从0.5提高到0.6-0.7）。呈现**混淆矩阵**，期望II级被误分类的比例下降，例如II级中分类正确的占比>70%（相比单模态可能只有60%左右）。还应计算**Kappa一致性系数**，反映模型分级结果与病理的吻合度，争取达到0.7以上（较高一致性）。在结果分析中，如发现大部分I级错分为II级，说明模型对极少样本的I级仍难以学习，可在讨论中指出这一局限。相比方向一，本方案的预期结果更贴近实际三分类需求：若能实现上述指标，已接近部分文献多模态模型在亚型分类中的性能（如某研究区分Luminal/非Luminal亚型准确率~88%, AUC ~0.96 ([Frontiers | Cross-attention multi-branch CNN using DCE-MRI to classify breast cancer molecular subtypes](https://www.frontiersin.org/journals/oncology/articles/10.3389/fonc.2023.1107850/full#:~:text=Results%3A%20Several%20classical%20deep%20learning,recall%20of%2093.33))），证明模型具备较强判别力。需要强调我们在独立验证集上的表现，以展示泛化能力。如果可能，进行**临床医生 vs. 模型**的对比实验也是有意义的：例如请经验丰富的影像科医师根据三种影像判断肿瘤等级，与模型输出比较，展示模型在一定程度上可以接近甚至超越人眼判断的一致性。
    
- **当前方法的难点与创新点**：多模态三分类的主要难点在于**数据获取与配准**。需要每例患者都有高质量的MG、US、MRI影像，这在现实中可能由于检查手段差异并不完全匹配，因此在数据层面需要创新的样本整合（例如利用缺失模态的病例进行半监督训练，虽然本方向未详细展开）。其次，**模型结构复杂度**和**训练难度**显著提高，多分支网络需要较长时间调参和训练，易出现某模态主导、另一些模态梯度消失的情况。为此可创新地引入例如梯度均衡损失，或设计模态Dropout（随机禁用某模态分支训练）以增强每个分支的独立辨识能力。**创新点**方面，本方案将晚期融合用于乳腺癌组织学三分类这是较新的探索。在以往研究中，多模态融合多用于良恶性或二分类预测，我们的方案把它扩展到更细粒度的分级任务，并提出了针对II级（中间类别）提高识别的策略。这有望填补多模态方法在乳腺癌分级领域的空白。另一个创新是**可解释性增强**：可在模型中集成Grad-CAM++等模块，对多模态输入分别生成注意力热图，观察模型关注的MG微钙化、US回声模式、MRI强化特征是否与高级别病变相符。这不仅验证融合模型的合理性，也是文章的一大亮点。综上，方向二通过融合多源信息力图显著提高分级预测性能，其挑战在于技术复杂度，但成功将意味着朝实用的全自动乳腺癌分级更近一步。
    

## 研究方向三：**基于先进架构和自监督预训练的多模态深度分级模型**

- **研究思路与目标**：在前两方向基础上，进一步提升模型性能和创新性，引入**更先进的网络架构**（如Transformer、Vision Transformer与CNN结合）以及**自监督预训练**等前沿技术，构建一个高精度、高鲁棒性的模型。该模型同样处理MG、US、MRI多模态数据，但在融合方式上尝试更深度的融合和注意力机制，使模型能够自主选择最有判别力的模态特征。此外，通过利用未标注影像进行自监督学习预训练，缓解标注依赖，提高模型在有限标注数据下的表现。目标是在三分类任务上逼近目前文献报道的最高水平（如AUC>0.95，III级判别接近完美），并在方法上有所创新，可投稿顶级期刊。
    
- **数据预处理与样本利用策略**：本方向充分利用**未标注的原始影像**资源。假设我们的“私人数据”不仅有标注了组织学级别的病例，也有相当数量未有病理分级标签的影像。首先，对所有带标签的病例进行和方向二类似的预处理（ROI提取、多模态配对）。然后，对**未标注数据**应用自监督预训练任务，这些数据可以包括有影像但尚未确定分级的病例，甚至包括健康对照等。自监督任务的设计需结合乳腺影像特点，例如：对MRI，可以采用重建式任务（masked autoencoder，即随机遮盖一部分MRI切片像素，训练模型重建完整影像）；对MG和US，可以采用对比学习（Random augment两张不同视角的增强图像，模型学习识别它们属于同一实例）。通过这些任务训练一个多模态编码器，使其学到乳腺影像的通用表示。在预训练过程中，可能需要一个**多模态融合编码架构**：例如使用Vision Transformer（ViT）处理图像块序列，再通过多头注意力将MG/US/MRI不同模态视为不同“视角”来融合。这一步不需要标签，因此可用全部数据来训练。预训练完成后，将模型fine-tune用于有标签的数据做分级预测。为充分利用样本，本方向在有标签数据的划分上会采用**分层K折交叉验证**，并可能进行**集成训练**（如训练多个不同初始化的模型取平均）来榨取每个病例的信息。此外，由于我们的模型复杂，需非常注重防止过拟合，可能采取**嵌套交叉验证**调参或者贝叶斯优化超参数，从而把有限标注数据价值发挥到最大。
    
- **模型结构与融合策略**：本方案采用**多模态Transformer融合架构**。例如设计一个包含CNN和Transformer混合模块的网络：首先，各模态图像经过各自的浅层卷积提取低级特征（例如ConvNeXt块提取纹理特征 ( [Predicting Breast Tumor Malignancy Using Deep ConvNeXt Radiomics and Quality-Based Score Pooling in Ultrasound Sequences - PMC](https://pmc.ncbi.nlm.nih.gov/articles/PMC9139635/#:~:text=breast%20cancer%20tumors%20as%20benign,four%20stages%2C%20where%20the%20SCR) )）；然后将提取的特征映射为一系列token序列输入Transformer。在Transformer层中，引入**交叉注意力机制**使得不同模态的token可以彼此交互融合（类似于Cross-Attention Multi-Branch Network的思想 ([Frontiers | Cross-attention multi-branch CNN using DCE-MRI to classify breast cancer molecular subtypes](https://www.frontiersin.org/journals/oncology/articles/10.3389/fonc.2023.1107850/full#:~:text=shown%20in%20Figure%202,explain%20these%20modules%20in%20detail))）。具体实现上，可为每个模态增加一个**模态编码标识**，并将所有模态的tokens拼接输入一个多头注意力Transformer，使模型自行在注意力权重中决定关注哪种模态的哪些特征。Transformer输出经过池化得到一个融合表示，再接若干全连接层完成分类。此外，还可以在模型中引入**多任务学习**分支：例如同时预测肿瘤的ER/PR/HER2分子分型（如果有数据）或者良恶性，这作为辅助任务在训练中共享部分网络，提高主任务的特征有效性。这种结构远比前两方向复杂，但有望更充分地融合多模态信息。为了稳定训练，我们也可以分阶段：先训练CNN部分的自监督，再加Transformer，再最后加分类头。融合策略上，本方案更接近**中期融合**（在Transformer中融合）而非简单晚期融合，预期比方向二的直连拼接能够捕获更细粒度的跨模态关联，如MG的微钙化模式与MRI的早期快速强化是否共同出现，用于判断III级。与此同时，模型也可以输出注意力图，表示每种模态对最终决策的贡献，这提升了融合过程的透明度。
    
- **PyTorch实现路径与计算资源评估**：实现该复杂架构需要较强的编程和调试能力。可以考虑使用现有的多模态Transformer框架库（如`transformers`或`timm`提供的ViT模型）并加以定制。首先实现自监督部分：例如使用Vision Transformer (ViT)结构，修改为适应多模态输入。PyTorch中可自定义一个ViT类，接受联合的多模态patch序列。自监督任务如MAE重建，则需要构建编码器-解码器架构，用Mask策略遮盖输入的一部分，并计算重建误差。也可尝试SimCLR之类的框架用于对比学习。完成预训练后，将编码器部分权重迁移到下游分级模型中。下游模型可另建一个包含Transformer Encoder和分类头的模块，初始化时加载预训练编码器参数。训练如此复杂模型最好在**多GPU并行**环境下进行，以减少单卡负荷。例如使用2~4块GPU同步BN、数据并行训练。如果多GPU不可得，也至少需要一张高内存GPU（如Tesla A100 40GB）来容纳大型Transformer的计算图。预计计算资源需求很高：自监督预训练可能要在数万张影像上迭代上百epoch，每epoch耗时数小时；下游微调相对快一些，但总训练时间可能以天计。存储方面，要保存自监督模型和微调模型参数，磁盘空间也需充足。鉴于此，本方向实现需要良好的工程优化，例如使用**混合精度**(AMP)加速，分布式训练缩短时间，以及检查点续训机制。团队也需有投入GPU算力的准备。尽管资源需求大，但如果成功，其成果将具有开创性。
    
- **模型训练策略**：训练分两阶段：**自监督预训练**和**有监督微调**。预训练阶段，不需要标签，训练目标根据选定的任务而定。以MAE为例：设置较高的mask比率（如75%），优化重建Loss，使模型学会从残缺的影像token推测缺失部分。这会强迫模型关注影像的全局结构和关键模式。另一种对比学习思路则要构造正负对，如随机挑选同一图像的不同增强版本为正对，比不同病例组合区分。无论哪种，需要训练足够轮次直到损失收敛并且观察到模型输出的特征对下游任务有判别力（可以在少量有标签数据上验证）。微调阶段，将预训练得到的编码器融合模型用于分级任务训练。因为预训练提供了一个良好的初始化，所以微调时可使用**较小的学习率**（如1e-5量级）训练Transformer部分，同时**联合训练CNN特征提取部分**。在这个阶段，可以继续混合加入少量自监督loss以防止模型忘掉预训练知识（一种类似于正则化的方法，称为**渐进解冻**和**损失混合**）。另外，因为模型复杂，更需要强力正则化：如**Dropout**（在Transformer多头注意力和全连接层后加入dropout=0.2~0.5），**L2权重衰减**增加泛化，还有**早停**监控防止过拟合陡增。针对三分类目标，仍采用加权损失和关注难分类样本的策略。可以动态调整类别权重（根据每轮训练后剩余误差调整）。在模型训练尾声，可能采用**集成模型**提高稳定性：例如保存若干epoch中表现最好的几个模型（依据验证集AUC），最后将它们的预测结果平均作为最终模型输出。这在文献中被证明可以提升可信度。总之，训练策略需要灵活调整，在保障主任务性能的同时，不丢弃预训练获得的跨模态知识。
    
- **模型评估方法与预期结果**：评价上，与方向二类似但期望更高。首先在验证集上观察指标，如模型是否在各类别上均取得了提升。最终在测试集上报告三分类的全面结果。**总体准确率**有望超过90%，这是一个非常高的水平，意味着很少有分类错误。**宏平均F1-score**希望在0.85以上，其中III级的F1可能最高（>0.9），I级次之但即便样本少也力争F1>0.8。**AUC**方面，每一类的一对多AUC尽量接近1.0，尤其III级 vs 其他有可能达到0.95以上，I级 vs 其他可能也可提升到0.9左右，平均AUC达到0.93～0.95。这将明显优于目前大多数放射组学模型（AUC 0.7-0.8）和已有深度学习模型报告的水平 ([Molecular-subtype guided automatic invasive breast cancer grading using dynamic contrast-enhanced MRI - PubMed](https://pubmed.ncbi.nlm.nih.gov/37716219#:~:text=Results%3A%20%20The%20molecular,weighted))。甚至可以尝试计算**逐类别ROC曲线的Delong检验**，与前一方向模型比较是否有显著差异。如果显著更高，统计学上证明本方案有效。除了传统指标，我们还会展示**决策曲线分析（DCA）**，评估在不同风险阈值下模型带来的净获益，例如当阈值设为需要将III级几率>50%才强化治疗时，模型判断的决策收益。期望我们的模型曲线在合理阈值范围内都高于“全治疗”或“无治疗”策略，证明其实用价值。另外，通过注意力可视化，我们将提供一些案例分析：例如模型成功识别某I级肿瘤的依据可能是MG上规则的圆形肿块（提示低级别）、US上均匀内部回声，这些都被注意力机制捕获；又如一个III级病例模型主要关注了其MRI上不均匀的快速强化区域和MG上的簇状钙化，从而高置信度预测为高级别。这些例子将以图表形式给出，增加论文说服力。**模型对比**也是评估一部分：将本模型与方向一的单模态CNN、方向二的简单融合CNN，以及传统放射组学方法进行对比，展示我们的改进幅度。如本模型准确率较单模态提高了X个百分点，AUC提高了约0.1-0.2。这些定量对比将凸显先进方法的优越性。
    
- **当前方法的难点与创新点**：方向三集合了多个前沿要素，因此难点不少。**首要难点**是模型规模大且训练步骤复杂，自监督预训练本身就需要大量实验去找到合适的任务和参数，同时确保预训练结果对下游有帮助。需要创新地设计适合乳腺多模态的数据增益策略，比如MG与MRI如何共同参与对比学习，这是较新的课题。**第二难点**在于Transformer的引入，可能带来训练不稳定（比如梯度消失或过拟合小数据）。我们通过分阶段和正则化应对，但仍需调试注意力模块以确保不同模态间的平衡，不致一种模态完全淹没其他。**第三**，多模态数据的质量和一致性要求更高，我们可能需要处理一些模态缺失或图像品质差的问题，这里可以创新性地利用预训练模型去推理出缺失模态的伪造特征（如训练一个网络预测US特征从MRI输入）。**创新点**方面，本方案在学术上最大的亮点是**自监督多模态学习在乳腺癌分级中的应用**。目前鲜有文献将自监督学习用于将MG/US/MRI结合起来训练，我们的尝试将拓展这一领域并有望取得性能突破。其次，**Transformer交叉模态融合**也是创新之举，它可自动捕获复杂的跨模态相关性，相比以往手工设计的融合策略更加智能，有潜力发现新的影像学标志。例如模型可能注意到“US上肿块边界模糊且MRI上KEP（扩散参数）值低”的组合特征对应于II级，这种关联可能未被之前的单模态模型识别过。我们的模型可以提供这些新发现。最后，在结果呈现上，我们计划提供完整的**开源代码和模型参数**，以推动领域研究的可复现性——这本身也是科研创新文化的一部分。虽然方向三实现难度最大，但若成功，将在乳腺癌影像组学领域树立新的标杆，为利用多模态深度学习全面表征肿瘤恶性程度提供宝贵经验，有望发表在高影响力SCI期刊上。
    

---

综上所述，通过循序渐进的三个研究方向，从单模态小样本深度学习模型到多模态融合模型再到结合自监督和新架构的高阶模型，我们既综述了当前乳腺癌组织学分级影像研究的方法学要点，也据此提出了创新方案。每一方向均针对当前研究的不足和挑战进行了优化设计，并结合文献证据和实际考虑制定了详细的实现与评估计划。这些方向的开展将有助于推动乳腺癌无创分级诊断技术的发展，为个体化治疗决策提供更可靠的依据。